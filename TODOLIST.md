TODOLIST:

Week of July 6
- combine 3-3 and ALL, see which edges appear in one or both
- generate top 10, used "aggregated" graphs
- investigate into presolving for scores faster, try to get optimal scores (try BDM scoring?)

Week of June 10
- investigate combining various datasets (e.g. all of the ECLIPSEX_BURSTX_GAPX) into a single dataset by adding additional variables that capture the differences
	- add gap size and burst size as columns, and combine
		- combine 1-3, 4-6, 7-10 for burst & gap size
		- work with /weekly/classes data for now
		- DONT combine datasets from different versions (e.g. ECLIPSE20 + ECLIPSE21), as this will double count many changes (duplicates)
~~- add LOC (lines of code) as a column to datasets~~
- proper discretization (instead of quartiling) all data columns:
	- plot histograms/frequency distributions of all columns to consider
	- consider common patterns:
		- top 5%, middle 90%, bottom 5%
		- zeros, top 5%, leftovers
		- zero, one, > 1 
- investigate removing columns that are correlated - e.g. NumberOfChanges vs NumberOfChangesLate vs NumberOfChangesEarly
	- look into using a correlation matrix to determine which to remove
- compare ECLIPSE dataset with other datasets
- folder structure documentation
- compare BN from optimal predictor dataset (gap size=3, burst size=3) with BN from general/aggregated dataset


Week of June 3rd
~~-generate top 100 BNs for a specific dataset, aggregate that
	- make sure all BNs have a similar score, otherwise discard~~ - experiment_2
~~-try to treat edges as undirected (for experiment_1 and experiment_2) see if that simplifies the aggregated structure~~

Week of May 27th, May 23rd
~~run & generate from different datasets, see if patterns persists in graphs~~ - experiment_1

investigate getting weights onto edges
investigate using conditional probability values as weights (instead of the ones generated by GOBNILP)
investigate ways to measure how similar graphs are, software to pick out patterns in multiple graphs
investigate using weka to preprocess and optimally discretize data
play around with variations on columns to include/exclude, classes vs packages
try GOBNILP with different scoring metrics (BIC vs BDM)
read up about conditional independence assumptions
